{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa5aef3-7ee9-4a31-90f8-8f44299f422c",
   "metadata": {},
   "source": [
    "# DENOISING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84497a5f-a0fc-40d1-bd50-afff73604408",
   "metadata": {},
   "source": [
    "La reducción de ruido es un proceso que consiste en eliminar el ruido de una señal. Existen técnicas de reducción de ruido para audio e imágenes, cuyos algoritmos tienden a alterar las señales en mayor o menor grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d735dcc-bc7a-4785-ad6a-2c15c8ab56fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Carga de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dfcab5-6c27-49ee-9b6a-c5baba8b6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\Ramón Sieira\\AppData\\Roaming\\Python\\Python39\\site-packages\\nnAudio\\Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1.12.1', '1.21.5', '1.4.4')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, random, time, sys, pickle, gc\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa, librosa.display\n",
    "import soundfile as sf\n",
    "from scipy.optimize import minimize\n",
    "import ast\n",
    "from metrics import AudioMetrics\n",
    "from metrics import AudioMetrics2\n",
    "from scipy import signal\n",
    "import wavfile\n",
    "from nnAudio.Spectrogram import CQT1992v2\n",
    "import noise_addition_utils\n",
    "import colorednoise as cn\n",
    "import cv2\n",
    "import audioread\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import IPython\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, WeightedRandomSampler\n",
    "import torch.utils.data as torchdata\n",
    "# import torch.optim as optim\n",
    "import torch_optimizer as optim\n",
    "from torchvision import transforms, models, datasets\n",
    "# from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "# from torchlibrosa.augmentation import SpecAugmentation\n",
    "from torchaudio.transforms import AmplitudeToDB\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import pywt\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay, confusion_matrix, mean_squared_error, precision_recall_fscore_support\n",
    "from skimage.transform import resize\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms as T\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import timm\n",
    "from timm.models.efficientnet import tf_efficientnet_b0_ns, tf_efficientnet_b1_ns, tf_efficientnet_b2_ns, tf_efficientnet_b3_ns, tf_efficientnet_b4_ns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.__version__, np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724b1b64-1c92-466e-94c7-54df7f1460f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tramo_length = 2.0\n",
    "  \n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dc32d-7997-40cf-8074-a8c64243e600",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creacion de la BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278468bf-5edf-4234-808a-da5c515507f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5459, 5)\n",
      "(5423, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 452/452 [00:17<00:00, 26.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 452/452 [00:03<00:00, 128.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "      <th>len_audio</th>\n",
       "      <th>sr_audio</th>\n",
       "      <th>seconds_audio</th>\n",
       "      <th>dir</th>\n",
       "      <th>clase</th>\n",
       "      <th>clase_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>012213d40662f86e8f775379bbf94dec</td>\n",
       "      <td>61.186</td>\n",
       "      <td>2.500</td>\n",
       "      <td>63.686</td>\n",
       "      <td>whistle</td>\n",
       "      <td>14700000</td>\n",
       "      <td>50000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>../input/datathon2022/dataset1/012213d40662f86...</td>\n",
       "      <td>whistle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0159af9f3ca04ada46f0c353ac210bb8</td>\n",
       "      <td>76.260</td>\n",
       "      <td>17.000</td>\n",
       "      <td>93.260</td>\n",
       "      <td>cetaceans_allfreq</td>\n",
       "      <td>14700000</td>\n",
       "      <td>50000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>../input/datathon2022/dataset1/0159af9f3ca04ad...</td>\n",
       "      <td>cetaceans_allfreq</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0159af9f3ca04ada46f0c353ac210bb8</td>\n",
       "      <td>161.482</td>\n",
       "      <td>1.000</td>\n",
       "      <td>162.482</td>\n",
       "      <td>click</td>\n",
       "      <td>14700000</td>\n",
       "      <td>50000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>../input/datathon2022/dataset1/0159af9f3ca04ad...</td>\n",
       "      <td>click</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0271a0818427d7fae7afde67bf49ba30</td>\n",
       "      <td>91.563</td>\n",
       "      <td>1.000</td>\n",
       "      <td>92.563</td>\n",
       "      <td>click</td>\n",
       "      <td>14700000</td>\n",
       "      <td>50000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>../input/datathon2022/dataset1/0271a0818427d7f...</td>\n",
       "      <td>click</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0271a0818427d7fae7afde67bf49ba30</td>\n",
       "      <td>125.627</td>\n",
       "      <td>1.316</td>\n",
       "      <td>126.943</td>\n",
       "      <td>click</td>\n",
       "      <td>14700000</td>\n",
       "      <td>50000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>../input/datathon2022/dataset1/0271a0818427d7f...</td>\n",
       "      <td>click</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path    start  duration      end              label  len_audio  sr_audio  seconds_audio                                                dir              clase  clase_num\n",
       "0  012213d40662f86e8f775379bbf94dec   61.186     2.500   63.686            whistle   14700000     50000          294.0  ../input/datathon2022/dataset1/012213d40662f86...            whistle          0\n",
       "1  0159af9f3ca04ada46f0c353ac210bb8   76.260    17.000   93.260  cetaceans_allfreq   14700000     50000          294.0  ../input/datathon2022/dataset1/0159af9f3ca04ad...  cetaceans_allfreq          2\n",
       "2  0159af9f3ca04ada46f0c353ac210bb8  161.482     1.000  162.482              click   14700000     50000          294.0  ../input/datathon2022/dataset1/0159af9f3ca04ad...              click          1\n",
       "3  0271a0818427d7fae7afde67bf49ba30   91.563     1.000   92.563              click   14700000     50000          294.0  ../input/datathon2022/dataset1/0271a0818427d7f...              click          1\n",
       "4  0271a0818427d7fae7afde67bf49ba30  125.627     1.316  126.943              click   14700000     50000          294.0  ../input/datathon2022/dataset1/0271a0818427d7f...              click          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    dataset1 = pd.read_csv(\"https://storage.googleapis.com/datathon2022/labels_dataset1_v2.csv\")\n",
    "    print(dataset1.shape)\n",
    "    df = dataset1.query('label!=\"volcano\"').drop_duplicates()\n",
    "    print(df.shape)\n",
    "    df.label.value_counts()\n",
    "\n",
    "    len_audio = []\n",
    "    sr_audio = []\n",
    "    seconds_audio = []\n",
    "    for name_file in tqdm(df.path.unique()):\n",
    "        y, sr = sf.read(f'../input/datathon2022/dataset1/{name_file}.wav')\n",
    "        len_audio.append(len(y))\n",
    "        sr_audio.append(sr)\n",
    "        seconds_audio.append(len(y)/sr)\n",
    "    df['len_audio'] = df['path'].map(dict(zip(df.path.unique(), len_audio)))\n",
    "    df['sr_audio'] = df['path'].map(dict(zip(df.path.unique(), sr_audio)))\n",
    "    df['seconds_audio'] = df['path'].map(dict(zip(df.path.unique(), seconds_audio)))\n",
    "    df['dir'] = ['../input/datathon2022/dataset1/'+i+'.wav' for i in df['path']]\n",
    "    df['clase'] = df['label'].values\n",
    "    clases = dict(df['clase'].value_counts())\n",
    "    clases_num = dict(zip(list(clases.keys()), np.arange(len(clases))))\n",
    "    df['clase_num'] = df['clase'].map(clases_num)\n",
    "    labels = pd.get_dummies(df['clase'])\n",
    "    \n",
    "    db_troceada = []\n",
    "    labels_troceada = []\n",
    "    step_audio = int(tramo_length * 50000)\n",
    "    for namefile in tqdm(df['path'].unique()):\n",
    "        df_file = df.query(f'path==\"{namefile}\"').reset_index()\n",
    "        df_file['start_pos'] = df_file[['start']].values * 50000\n",
    "        df_file['end_pos'] = df_file[['end']].values * 50000\n",
    "        df_file['start_pos']  = df_file['start_pos'].astype(int)\n",
    "        df_file['end_pos']  = df_file['end_pos'].astype(int)\n",
    "        for pos_ini in np.arange(0,14700000,step_audio):\n",
    "            pos_fin = pos_ini + step_audio\n",
    "            if pos_fin > 14700000:\n",
    "                pos_fin = 14700000\n",
    "            pos_dentro = (pos_ini>df_file['start_pos'].values) & (pos_ini<df_file['end_pos'].values)\n",
    "            labels_list = []\n",
    "            if np.sum(pos_dentro>0):\n",
    "                labels_list = list(np.unique(df_file.loc[pos_dentro, 'label'].values))\n",
    "#                 print(namefile, pos_ini, pos_fin, labels)\n",
    "            db_troceada.append(dict(namefile=namefile,\n",
    "                                    dir=f'../input/datathon2022/dataset1/{namefile}.wav',\n",
    "                                    start=float(pos_ini/50000),\n",
    "                                    end=float(pos_fin/50000),\n",
    "                                    pos_ini=pos_ini,\n",
    "                                    pos_fin=pos_fin,\n",
    "                                    labels=labels_list))\n",
    "            labels_row = np.zeros(len(clases_num))\n",
    "            for label in labels_list:\n",
    "                labels_row[clases_num[label]] = 1.0\n",
    "            labels_troceada.append(labels_row)\n",
    "            \n",
    "    db_troceada = pd.DataFrame(db_troceada)\n",
    "    labels_troceada = pd.DataFrame(np.stack(labels_troceada))\n",
    "    labels_troceada.columns = list(clases_num.keys())\n",
    "\n",
    "    with open('../input/database_datathon2022.pickle', 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        pickle.dump(clases, f)\n",
    "        pickle.dump(clases_num, f)\n",
    "        pickle.dump(labels, f)\n",
    "        pickle.dump(db_troceada, f)\n",
    "        pickle.dump(labels_troceada, f)\n",
    "else:\n",
    "    with open('../input/database_datathon2022.pickle', 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        clases = pickle.load(f)\n",
    "        clases_num = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "        db_troceada = pickle.load(f)\n",
    "        labels_troceada = pickle.load(f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1886e2dd-b201-4c6d-a691-a5422743f898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66444, 7), (66444, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_orig = db_troceada.reset_index(drop=True)\n",
    "labels_train_orig = labels_troceada\n",
    "df_train_orig.shape, labels_train_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05ab4b-91aa-400a-b69d-acbdf9a6db9b",
   "metadata": {},
   "source": [
    "Aqui se ven los datos separados donde hay o no evento es decir ruido o evento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550472af-f1a6-41f0-b41d-a3876fd29931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    58611\n",
       "True      7833\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hay_evento = (labels_train_orig.sum(axis=1)>0)\n",
    "hay_evento.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea5700-8543-440a-b4cb-72098ca3ef25",
   "metadata": {},
   "source": [
    "Sample del dataframe, como vemos puede haber labels a la vez para un mismo intervalo, en este caso hemos particionado los eventos/no eventos en archivos de 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db49e02-b7fc-491b-852b-d0410f497152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namefile</th>\n",
       "      <th>dir</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pos_ini</th>\n",
       "      <th>pos_fin</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21635</th>\n",
       "      <td>5849e10d3febc43b576290f2f6706ddd</td>\n",
       "      <td>../input/datathon2022/dataset1/5849e10d3febc43...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2600000</td>\n",
       "      <td>2700000</td>\n",
       "      <td>[cetaceans_allfreq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>083edafa2b1d1f1944018ca7d2e28e97</td>\n",
       "      <td>../input/datathon2022/dataset1/083edafa2b1d1f1...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5500000</td>\n",
       "      <td>5600000</td>\n",
       "      <td>[allfreq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57920</th>\n",
       "      <td>ddc7eeb968642eb5fc9f28fe3fd9e718</td>\n",
       "      <td>../input/datathon2022/dataset1/ddc7eeb968642eb...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>200000</td>\n",
       "      <td>300000</td>\n",
       "      <td>[whistle]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               namefile                                                dir  start    end  pos_ini  pos_fin               labels\n",
       "21635  5849e10d3febc43b576290f2f6706ddd  ../input/datathon2022/dataset1/5849e10d3febc43...   52.0   54.0  2600000  2700000  [cetaceans_allfreq]\n",
       "1672   083edafa2b1d1f1944018ca7d2e28e97  ../input/datathon2022/dataset1/083edafa2b1d1f1...  110.0  112.0  5500000  5600000            [allfreq]\n",
       "57920  ddc7eeb968642eb5fc9f28fe3fd9e718  ../input/datathon2022/dataset1/ddc7eeb968642eb...    4.0    6.0   200000   300000            [whistle]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_orig.loc[hay_evento].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29616c46-2602-4482-b065-13fab26170b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MODELO DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d23da-a0e1-49a5-ae72-c7e72426125d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creacion del Dataset de entramiento train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fcf76-6e7a-4e24-adab-67cd5513ee36",
   "metadata": {},
   "source": [
    "Para realizar el entrenamiento generamos 1000 archivos por cada seccion de entrada/train-salida/train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2917b3f8-71b8-45b9-a1a8-04fbee59c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n"
     ]
    }
   ],
   "source": [
    "unicos_names = df_train_orig.namefile.unique()\n",
    "print(len(unicos_names))\n",
    "primeros = unicos_names[:100]\n",
    "segundos = unicos_names[100:200]\n",
    "terceros = unicos_names[200:300]\n",
    "noeventos = df_train_orig.loc[~hay_evento]\n",
    "\n",
    "df1 = noeventos[noeventos.namefile.isin(primeros)].sample(100)\n",
    "df2 = noeventos[noeventos.namefile.isin(segundos)].sample(100)\n",
    "df3 = noeventos[noeventos.namefile.isin(terceros)].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0686fec2-65cb-4630-ab49-023793f781d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 7), (100, 7), (100, 7))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6affda-96e5-47f0-96e0-303d25005c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Almacenamiento de los recortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a651762e-a3b7-4e6b-9ac3-426b9b2c8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:04, 24.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for nrow, record in tqdm(df1.loc[~hay_evento].iterrows()):\n",
    "    path = record['dir']\n",
    "    namefile = record['namefile']+'_'+str(record['pos_ini'])\n",
    "    y, sr = sf.read(path)\n",
    "    y = y[record['pos_ini']:record['pos_fin']]\n",
    "    sf.write('../Noise2Noise-audio_denoising_without_clean_training_data-main/Datasets/noise_marine_train_salida/'+namefile+'.wav',\n",
    "             y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d2eab6-4420-4944-be76-e86b81fd489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:04, 24.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for nrow, record in tqdm(df2.loc[~hay_evento].iterrows()):\n",
    "    path = record['dir']\n",
    "    namefile = record['namefile']+'_'+str(record['pos_ini'])\n",
    "    y, sr = sf.read(path)\n",
    "    y = y[record['pos_ini']:record['pos_fin']]\n",
    "    sf.write('../Noise2Noise-audio_denoising_without_clean_training_data-main/Datasets/noise_marine_train_entrada/'+namefile+'.wav',\n",
    "             y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4792409-525c-42f1-8337-dd790316d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:04, 24.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for nrow, record in tqdm(df3.loc[~hay_evento].iterrows()):\n",
    "    path = record['dir']\n",
    "    namefile = record['namefile']+'_'+str(record['pos_ini'])\n",
    "    y, sr = sf.read(path)\n",
    "    y = y[record['pos_ini']:record['pos_fin']]\n",
    "    sf.write('../Noise2Noise-audio_denoising_without_clean_training_data-main/Datasets/noise_marine_test/'+namefile+'.wav',\n",
    "             y, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5aa27d-b9ee-45ff-a090-8620a5a7c6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creacion/Definicion del modelo + Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f6e893-16a3-4f0c-b1d1-6f2af4642e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_class = \"marine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22bce6b2-8c28-41b9-a5cd-efa41f8f3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_type =  \"Noise2Noise\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e35552-993d-4630-9ba2-854d6e554336",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DIR = Path('Datasets/noise_marine_train_entrada')\n",
    "TRAIN_TARGET_DIR = Path('Datasets/noise_marine_train_salida/')\n",
    "TEST_NOISY_DIR = Path('Datasets/noise_marine_test/')\n",
    "TEST_CLEAN_DIR = Path('Datasets/noise_marine_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c3facc-d2e2-4e86-808f-7757253e54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = str(noise_class)+\"_\"+training_type\n",
    "os.makedirs(basepath,exist_ok=True)\n",
    "os.makedirs(basepath+\"/Weights\",exist_ok=True)\n",
    "os.makedirs(basepath+\"/Samples\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5888386f-46ea-4917-9373-7df47bfb3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7180ff-e44a-4736-b4d8-a5f6409df49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000 \n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5f12e9-ea3b-40d7-8b68-ae93977341c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, noisy_files, clean_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        # list of files\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.clean_files = sorted(clean_files)\n",
    "        \n",
    "        # stft parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        # fixed len\n",
    "        self.max_len = 165000\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load to tensors and normalization\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "        \n",
    "        # padding/cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "        \n",
    "        # Short-time Fourier transform\n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        x_clean_stft = torch.stft(input=x_clean, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return x_noisy_stft, x_clean_stft\n",
    "        \n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        current_len = waveform.shape[1]\n",
    "        \n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        output[0, -current_len:] = waveform[0, :self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13b8533a-61c2-4fd0-97dc-51b108fd325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Training files: 100\n",
      "No. of Testing files: 100\n"
     ]
    }
   ],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob('*.wav')))[:200]\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob('*.wav')))[:200]\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))[:200]\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))[:200]\n",
    "\n",
    "print(\"No. of Training files:\",len(train_input_files))\n",
    "print(\"No. of Testing files:\",len(test_noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c99bef77-7978-43e3-bea9-5d25f7c58b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_input_files, train_target_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9154aae9-185d-4701-ba64-db74dc19624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a003c71b-bc0f-4892-b7e6-b3aafb49eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_metrics(test_loader, model):\n",
    "    metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(len(metric_names))]\n",
    "    \n",
    "    for i,(noisy,clean) in enumerate(test_loader):\n",
    "        x_est = model(noisy.to(DEVICE), is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        x_c_np = torch.istft(torch.squeeze(clean[0], 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "        metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "        \n",
    "        overall_metrics[0].append(metrics.CSIG)\n",
    "        overall_metrics[1].append(metrics.CBAK)\n",
    "        overall_metrics[2].append(metrics.COVL)\n",
    "        overall_metrics[3].append(metrics.PESQ)\n",
    "        overall_metrics[4].append(metrics.SSNR)\n",
    "        overall_metrics[5].append(metrics.STOI)\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for i in range(len(metric_names)):\n",
    "        metrics_dict[metric_names[i]] ={'mean': np.mean(overall_metrics[i]), 'std_dev': np.std(overall_metrics[i])} \n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51a5f466-7a7b-473a-86a7-4276962bd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a745fa3b-5f0b-4cf1-a1e7-f214988693b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce90202-ea49-435e-96a2-a1194bcfc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f52083b-4889-4311-bae5-74ce36721c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39cb9cf2-7806-4f53-8a05-e45ce9773b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a307cbda-4520-4dbe-86b0-1603f2a96f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pesq import pesq\n",
    "from scipy import interpolate\n",
    "def resample(original, old_rate, new_rate):\n",
    "    if old_rate != new_rate:\n",
    "        duration = original.shape[0] / old_rate\n",
    "        time_old  = np.linspace(0, duration, original.shape[0])\n",
    "        time_new  = np.linspace(0, duration, int(original.shape[0] * new_rate / old_rate))\n",
    "        interpolator = interpolate.interp1d(time_old, original.T)\n",
    "        new_audio = interpolator(time_new).T\n",
    "        return new_audio\n",
    "    else:\n",
    "        return original\n",
    "\n",
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    # to time-domain waveform\n",
    "    y_true_ = torch.squeeze(y_true_, 1)\n",
    "    y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    x = torch.istft(x_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)\n",
    "\n",
    "wonky_samples = []\n",
    "\n",
    "def getMetricsonLoader(loader, net, use_net=True):\n",
    "    net.eval()\n",
    "    # Original test metrics\n",
    "    scale_factor = 32768\n",
    "    # metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\",\"SNR \"]\n",
    "    metric_names = [\"PESQ-WB\",\"PESQ-NB\",\"SNR\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(5)]\n",
    "    for i, data in enumerate(loader):\n",
    "        if (i+1)%10==0:\n",
    "            end_str = \"\\n\"\n",
    "        else:\n",
    "            end_str = \",\"\n",
    "        #print(i,end=end_str)\n",
    "        if i in wonky_samples:\n",
    "            print(\"Something's up with this sample. Passing...\")\n",
    "        else:\n",
    "            noisy = data[0]\n",
    "            clean = data[1]\n",
    "            if use_net: # Forward of net returns the istft version\n",
    "                x_est = net(noisy.to(DEVICE), is_istft=True)\n",
    "                x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
    "            else:\n",
    "                x_est_np = torch.istft(torch.squeeze(noisy, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            x_clean_np = torch.istft(torch.squeeze(clean, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "            \n",
    "        \n",
    "            metrics = AudioMetrics2(x_clean_np, x_est_np, 48000)\n",
    "            \n",
    "            ref_wb = resample(x_clean_np, 48000, 16000)\n",
    "            deg_wb = resample(x_est_np, 48000, 16000)\n",
    "            pesq_wb = pesq(16000, ref_wb, deg_wb, 'wb')\n",
    "            \n",
    "            ref_nb = resample(x_clean_np, 48000, 8000)\n",
    "            deg_nb = resample(x_est_np, 48000, 8000)\n",
    "            pesq_nb = pesq(8000, ref_nb, deg_nb, 'nb')\n",
    "\n",
    "            #print(new_scores)\n",
    "            #print(metrics.PESQ, metrics.STOI)\n",
    "\n",
    "            overall_metrics[0].append(pesq_wb)\n",
    "            overall_metrics[1].append(pesq_nb)\n",
    "            overall_metrics[2].append(metrics.SNR)\n",
    "            overall_metrics[3].append(metrics.SSNR)\n",
    "            overall_metrics[4].append(metrics.STOI)\n",
    "    print()\n",
    "    print(\"Sample metrics computed\")\n",
    "    results = {}\n",
    "    for i in range(5):\n",
    "        temp = {}\n",
    "        temp[\"Mean\"] =  np.mean(overall_metrics[i])\n",
    "        temp[\"STD\"]  =  np.std(overall_metrics[i])\n",
    "        temp[\"Min\"]  =  min(overall_metrics[i])\n",
    "        temp[\"Max\"]  =  max(overall_metrics[i])\n",
    "        results[metric_names[i]] = temp\n",
    "    print(\"Averages computed\")\n",
    "    if use_net:\n",
    "        addon = \"(cleaned by model)\"\n",
    "    else:\n",
    "        addon = \"(pre denoising)\"\n",
    "    print(\"Metrics on test data\",addon)\n",
    "    for i in range(5):\n",
    "        print(\"{} : {:.3f}+/-{:.3f}\".format(metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6b4dbd6-cea6-40c7-8faa-98dae1b0a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.\n",
    "    counter = 0\n",
    "    for noisy_x, clean_x in train_loader:\n",
    "\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "        # zero  gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_ep_loss += loss.item() \n",
    "        counter += 1\n",
    "\n",
    "    train_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_ep_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bce66be5-5bd4-4a1e-b86b-fd968271e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
    "    net.eval()\n",
    "    test_ep_loss = 0.\n",
    "    counter = 0.\n",
    "    \n",
    "    #print(\"Actual compute done...testing now\")\n",
    "    \n",
    "    testmet = getMetricsonLoader(test_loader,net,use_net)\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_ep_loss, testmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f9fbcf2-ecc9-4f6d-9825-a8b0b900277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "\n",
    "        # first evaluating for comparison\n",
    "        \n",
    "        if e == 0 and training_type==\"Noise2Clean\":\n",
    "            print(\"Pre-training evaluation\")\n",
    "            testmet = getMetricsonLoader(test_loader,net,False)    # again, modified cuz im loading\n",
    "        \n",
    "            with open(basepath + \"/results.txt\",\"w+\") as f:\n",
    "                f.write(\"Initial : \\n\")\n",
    "                f.write(str(testmet))\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        test_loss = 0\n",
    "        scheduler.step()\n",
    "        print(\"Saving model....\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_loss, testmet = test_epoch(net, test_loader, loss_fn,use_net=True)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #print(\"skipping testing cuz peak autism idk\")\n",
    "        \n",
    "        with open(basepath + \"/results.txt\",\"a\") as f:\n",
    "            f.write(\"Epoch :\"+str(e+1) + \"\\n\" + str(testmet))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        print(\"OPed to txt\")\n",
    "        \n",
    "        torch.save(net.state_dict(), basepath +'/Weights/dc20_model_'+str(e+1)+'.pth')\n",
    "        torch.save(optimizer.state_dict(), basepath+'/Weights/dc20_opt_'+str(e+1)+'.pth')\n",
    "        \n",
    "        print(\"Models saved\")\n",
    "\n",
    "        # clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca19ca37-8c74-456c-ac67-add61dabb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCUnet20(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.set_size(model_complexity=int(45//1.414), input_channels=1, model_depth=20)\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2\n",
    "        \n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
    "                             filter_size=self.enc_kernel_sizes[i], stride_size=self.enc_strides[i], padding=self.enc_paddings[i])\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i])\n",
    "            else:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i], last_layer=True)\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "       \n",
    "        \n",
    "    def forward(self, x, is_istft=True):\n",
    "        # print('x : ', x.shape)\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            # print('Encoder : ', x.shape)\n",
    "            \n",
    "        p = x\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            # print('Decoder : ', p.shape)\n",
    "            p = torch.cat([p, xs[self.model_length - 1 - i]], dim=1)\n",
    "        \n",
    "        # u9 - the mask\n",
    "        \n",
    "        mask = p\n",
    "        \n",
    "        # print('mask : ', mask.shape)\n",
    "        \n",
    "        output = mask * orig_x\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "\n",
    "        if is_istft:\n",
    "            output = torch.istft(output, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [input_channels,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 128]\n",
    "\n",
    "            self.enc_kernel_sizes = [(7, 1),\n",
    "                                     (1, 7),\n",
    "                                     (6, 4),\n",
    "                                     (7, 5),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3)]\n",
    "\n",
    "            self.enc_strides = [(1, 1),\n",
    "                                (1, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1)]\n",
    "\n",
    "            self.enc_paddings = [(3, 0),\n",
    "                                 (0, 3),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0)]\n",
    "\n",
    "            self.dec_channels = [0,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 1]\n",
    "\n",
    "            self.dec_kernel_sizes = [(6, 3), \n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (8, 5),\n",
    "                                     (7, 5),\n",
    "                                     (1, 7),\n",
    "                                     (7, 1)]\n",
    "\n",
    "            self.dec_strides = [(2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (1, 1),\n",
    "                                (1, 1)]\n",
    "\n",
    "            self.dec_paddings = [(0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 3),\n",
    "                                 (3, 0)]\n",
    "            \n",
    "            self.dec_output_padding = [(0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0)]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f80da8a-b463-4139-96c9-53e30a194247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "loss_fn = wsdr_fn\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28e54a-4df2-4447-b31e-818905b1b440",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "644046b4-82b1-446d-9855-3f2030eb70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▊                                                              | 1/4 [03:08<09:25, 188.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample metrics computed\n",
      "Averages computed\n",
      "Metrics on test data (cleaned by model)\n",
      "PESQ-WB : 3.462+/-0.845\n",
      "PESQ-NB : 4.199+/-0.352\n",
      "SNR : -3.571+/-0.740\n",
      "SSNR : -5.958+/-0.420\n",
      "STOI : 0.632+/-0.042\n",
      "OPed to txt\n",
      "Models saved\n",
      "Saving model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 2/4 [06:09<06:08, 184.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample metrics computed\n",
      "Averages computed\n",
      "Metrics on test data (cleaned by model)\n",
      "PESQ-WB : 2.404+/-0.689\n",
      "PESQ-NB : 3.409+/-0.481\n",
      "SNR : -5.140+/-0.379\n",
      "SSNR : -6.903+/-0.252\n",
      "STOI : 0.569+/-0.051\n",
      "OPed to txt\n",
      "Models saved\n",
      "Saving model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [09:11<03:02, 182.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample metrics computed\n",
      "Averages computed\n",
      "Metrics on test data (cleaned by model)\n",
      "PESQ-WB : 3.182+/-0.761\n",
      "PESQ-NB : 3.760+/-0.391\n",
      "SNR : -5.352+/-0.252\n",
      "SSNR : -7.095+/-0.170\n",
      "STOI : 0.597+/-0.039\n",
      "OPed to txt\n",
      "Models saved\n",
      "Saving model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [12:11<00:00, 182.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample metrics computed\n",
      "Averages computed\n",
      "Metrics on test data (cleaned by model)\n",
      "PESQ-WB : 3.156+/-0.768\n",
      "PESQ-NB : 3.759+/-0.393\n",
      "SNR : -5.345+/-0.259\n",
      "SSNR : -7.086+/-0.175\n",
      "STOI : 0.597+/-0.039\n",
      "OPed to txt\n",
      "Models saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da4fa92c-e3ee-41b1-b7cc-20035c2bccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = \"marine_Noise2Noise/Weights/dc20_model_4.pth\"\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "\n",
    "checkpoint = torch.load(model_weights_path,\n",
    "                        map_location=torch.device('cpu')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246b6d9-b053-42d9-96fd-31d4159718c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62a794e1-e637-4f1f-8d7e-d24df6f55667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06617025fc3b16ee10bae2755634fc81\n",
      "1947ce80f94154d6085084a574de5474\n",
      "231c5f04e68be49c0a052bf222f13ae9\n",
      "2df0aaeb652216469c343b814ec429cd\n",
      "3509010242589e4bf2e54239e08ff322\n",
      "42401006318d93fdd03eb88a67930a72\n",
      "5df9b4772b0b501e0fb2b3ec7f749771\n",
      "76302d5f26e67d091a6d6cb120fe78ae\n",
      "79c83705430206469aa5d88a4581c968\n",
      "8246a7991f4eead7094882dba0871b17\n",
      "9fb046485fa8745f908db86e2e89f8c6\n",
      "a2b2c78f2de1b6c29c5cac33b72366e3\n",
      "ab8b9a2196d6d96831fbf8cbd2d13a6c\n",
      "b1315e779778885a511668ceb349d32b\n",
      "b2cc12c06ee92cd1ee1ed452087f6136\n",
      "db581f1fe0ae54b48e0033d1f8bc509f\n",
      "e3c63621e843cf16dfff8f42bd6c9015\n",
      "e88555d41934d6c782cdf542bfbbf58b\n",
      "e91c8b374bcd86190632dd2206b7aedb\n",
      "f9175beb21dd1394a387af49ab505bcb\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "index = 0\n",
    "for a in os.listdir('marine_Noise2Noise/Samples/'):\n",
    "    if len(a.split('.'))==2:\n",
    "        print(a.split('.wav')[0])\n",
    "        os.mkdir('marine_Noise2Noise/Samples/'+str(a.split('.wav')[0]))\n",
    "    sound = AudioSegment.from_wav(\"marine_Noise2Noise/Samples/\"+a)\n",
    "    audio=[]\n",
    "    for b in range(0,len(sound),3400):\n",
    "        extract = sound[b:b+3400]\n",
    "        extract.export('marine_Noise2Noise/Samples/'+str(a.split('.wav')[0])+str('/'+str(b)+a.split('.wav')[0])+'.wav', format=\"wav\")\n",
    "        file=str(b)+a\n",
    "        test_noisy_files = sorted(list(Path(f\"marine_Noise2Noise/Samples/{a.split('.wav')[0]}/\").rglob(file)))\n",
    "        test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "        test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "        dcunet20.eval()\n",
    "        test_loader_single_unshuffled_iter = iter(test_loader_single_unshuffled)\n",
    "        x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "        for _ in range(index):\n",
    "            x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "        x_est = dcunet20(x_n.to(DEVICE), is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        audio.append(x_est_np)\n",
    "    path='marine_Noise2Noise/Samples/'+'/denoise'+str(a.split('.wav')[0])+'.wav'\n",
    "    noise_addition_utils.save_audio_file(np_array=audio,file_path=Path(path), sample_rate=SAMPLE_RATE, bit_precision=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41925237-2290-4bd3-9ee6-290d80c2ac3e",
   "metadata": {},
   "source": [
    "# Aplicacion de algoritmo clasico de reduccion de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1a8ccfd-0187-42a2-9f19-3f30dfb6676d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denoise06617025fc3b16ee10bae2755634fc81.wav',\n",
       " 'denoise1947ce80f94154d6085084a574de5474.wav',\n",
       " 'denoise231c5f04e68be49c0a052bf222f13ae9.wav',\n",
       " 'denoise2df0aaeb652216469c343b814ec429cd.wav',\n",
       " 'denoise3509010242589e4bf2e54239e08ff322.wav',\n",
       " 'denoise42401006318d93fdd03eb88a67930a72.wav',\n",
       " 'denoise5df9b4772b0b501e0fb2b3ec7f749771.wav',\n",
       " 'denoise76302d5f26e67d091a6d6cb120fe78ae.wav',\n",
       " 'denoise79c83705430206469aa5d88a4581c968.wav',\n",
       " 'denoise8246a7991f4eead7094882dba0871b17.wav',\n",
       " 'denoise9fb046485fa8745f908db86e2e89f8c6.wav',\n",
       " 'denoisea2b2c78f2de1b6c29c5cac33b72366e3.wav',\n",
       " 'denoiseab8b9a2196d6d96831fbf8cbd2d13a6c.wav',\n",
       " 'denoiseb1315e779778885a511668ceb349d32b.wav',\n",
       " 'denoiseb2cc12c06ee92cd1ee1ed452087f6136.wav',\n",
       " 'denoisedb581f1fe0ae54b48e0033d1f8bc509f.wav',\n",
       " 'denoisee3c63621e843cf16dfff8f42bd6c9015.wav',\n",
       " 'denoisee88555d41934d6c782cdf542bfbbf58b.wav',\n",
       " 'denoisee91c8b374bcd86190632dd2206b7aedb.wav',\n",
       " 'denoisef9175beb21dd1394a387af49ab505bcb.wav',\n",
       " 'e3c63621e843cf16dfff8f42bd6c9015.wav',\n",
       " 'e88555d41934d6c782cdf542bfbbf58b.wav',\n",
       " 'e91c8b374bcd86190632dd2206b7aedb.wav',\n",
       " 'f9175beb21dd1394a387af49ab505bcb.wav']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('marine_Noise2Noise/Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9577d670-c93c-4644-9953-f7a7c3f0a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59347f48-7d7b-4333-a141-dbfd6752abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marine_Noise2Noise/Samples/denoise06617025fc3b16ee10bae2755634fc81.wav\n",
      "marine_Noise2Noise/Samples/denoise1947ce80f94154d6085084a574de5474.wav\n",
      "marine_Noise2Noise/Samples/denoise231c5f04e68be49c0a052bf222f13ae9.wav\n",
      "marine_Noise2Noise/Samples/denoise2df0aaeb652216469c343b814ec429cd.wav\n",
      "marine_Noise2Noise/Samples/denoise3509010242589e4bf2e54239e08ff322.wav\n",
      "marine_Noise2Noise/Samples/denoise42401006318d93fdd03eb88a67930a72.wav\n",
      "marine_Noise2Noise/Samples/denoise5df9b4772b0b501e0fb2b3ec7f749771.wav\n",
      "marine_Noise2Noise/Samples/denoise76302d5f26e67d091a6d6cb120fe78ae.wav\n",
      "marine_Noise2Noise/Samples/denoise79c83705430206469aa5d88a4581c968.wav\n",
      "marine_Noise2Noise/Samples/denoise8246a7991f4eead7094882dba0871b17.wav\n",
      "marine_Noise2Noise/Samples/denoise9fb046485fa8745f908db86e2e89f8c6.wav\n",
      "marine_Noise2Noise/Samples/denoisea2b2c78f2de1b6c29c5cac33b72366e3.wav\n",
      "marine_Noise2Noise/Samples/denoiseab8b9a2196d6d96831fbf8cbd2d13a6c.wav\n",
      "marine_Noise2Noise/Samples/denoiseb1315e779778885a511668ceb349d32b.wav\n",
      "marine_Noise2Noise/Samples/denoiseb2cc12c06ee92cd1ee1ed452087f6136.wav\n",
      "marine_Noise2Noise/Samples/denoisedb581f1fe0ae54b48e0033d1f8bc509f.wav\n",
      "marine_Noise2Noise/Samples/denoisee3c63621e843cf16dfff8f42bd6c9015.wav\n",
      "marine_Noise2Noise/Samples/denoisee88555d41934d6c782cdf542bfbbf58b.wav\n",
      "marine_Noise2Noise/Samples/denoisee91c8b374bcd86190632dd2206b7aedb.wav\n",
      "marine_Noise2Noise/Samples/denoisef9175beb21dd1394a387af49ab505bcb.wav\n"
     ]
    }
   ],
   "source": [
    "for a in os.listdir('marine_Noise2Noise/Samples'):\n",
    "    if len(a.split('.'))==2:\n",
    "        print('marine_Noise2Noise/Samples/{}'.format(a))\n",
    "        sr,data = wavfile.read('marine_Noise2Noise/Samples/{}'.format(a))\n",
    "        data=data*4\n",
    "        reduced_noise = nr.reduce_noise(y=data, sr=sr,prop_decrease=0.7)\n",
    "        wavfile.write('marine_Noise2Noise/Samples/NONOISE_{}'.format(a), sr, reduced_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e7dd1-6da3-4995-82aa-4e7033bcf321",
   "metadata": {},
   "source": [
    "Los archivos NONOISE_DENOISE son aquellos pasados tanto por la red neuronal como por el procesimiento clasico, los DENOISE solo por la red neuronal, los NOISE solo por los metodos clasicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b054a8-f570-493a-b402-c96a446c475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cadf7-3e8c-44e4-bd7d-311adbc11e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534a630-f4a2-4b40-a8f3-49188856ea47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4ca10-3f62-431b-becf-1b5030500ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f8e99-603e-4e2b-be4e-922c674581ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00dabb-7617-43e6-af70-36ff2551aa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc687f4-7965-44b7-a17b-e178464f8358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a8a8b-dd26-4584-836a-500745061792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29570135-8b27-47ea-8e4d-0e3e6b187096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc35334-9bdb-4252-b41f-d6e7b44a06eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154af26-6ef2-4e04-842c-62afc8349131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1e4d7-1bee-4d68-907c-bd682a531a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fca431-6d07-4ee8-9abf-2cd92e999b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbe1e5-90eb-48b7-91fa-b566e27e3a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f6805-874e-4b85-9d0e-18c8bef09b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
